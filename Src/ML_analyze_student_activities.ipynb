{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from google.colab import files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "uploaded = files.upload()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the paths of original datasets\n",
    "data_path = '/content/data.csv'\n",
    "log_path = '/content/log.csv'\n",
    "data_path_2021 = '/content/data-2021.csv'\n",
    "log_path_2021 = '/content/log-2021.csv'\n",
    "\n",
    "# load data\n",
    "data_df = pd.read_csv(data_path)\n",
    "log_df = pd.read_csv(log_path)\n",
    "data_df_2021 = pd.read_csv(data_path_2021)\n",
    "log_df_2021 = pd.read_csv(log_path_2021)\n",
    "\n",
    "# rename the student in 2021\n",
    "data_df_2021['Student'] = data_df_2021['Student'] + '-2021'\n",
    "log_df_2021['Student'] = log_df_2021['Student'] + '-2021'\n",
    "\n",
    "# merge data\n",
    "data_df = pd.concat([data_df, data_df_2021])\n",
    "log_df = pd.concat([log_df, log_df_2021])\n",
    "\n",
    "columns_with_missing_values = ['Week 3 quiz', 'Week 4 quiz', 'Week 5 quiz', 'Week 6 quiz',\n",
    "       'Week 7 quiz', 'Week 8 quiz', 'Week 9 quiz', 'Week 10 quiz',\n",
    "       'Week 11 quiz', 'Week 12 quiz', 'Week 13 quiz', 'Week 14 quiz',\n",
    "       'Week 15 quiz', 'Week 16 quiz', 'Week 3 homework', 'Week 4 homework', 'Week 5 homework',\n",
    "       'Week 7 homework', 'Week 8 homework', 'Week 9 homework',\n",
    "       'Week 10 homework', 'Week 11 homework', 'Week 13 homework',\n",
    "       'Week 14 homework', 'Week 15 homework']\n",
    "columns_not_missing_values = ['Week 1 quiz', 'Week 2 quiz',\n",
    "       'Week 3 quiz', 'Week 4 quiz', 'Week 5 quiz', 'Week 6 quiz',\n",
    "       'Week 7 quiz', 'Week 8 quiz', 'Week 9 quiz', 'Week 10 quiz',\n",
    "       'Week 11 quiz', 'Week 12 quiz', 'Week 13 quiz', 'Week 14 quiz',\n",
    "       'Week 15 quiz', 'Week 16 quiz', 'Week 1 homework', 'Week 2 homework',\n",
    "       'Week 3 homework', 'Week 4 homework', 'Week 5 homework',\n",
    "       'Week 7 homework', 'Week 8 homework', 'Week 9 homework',\n",
    "       'Week 10 homework', 'Week 11 homework', 'Week 13 homework',\n",
    "       'Week 14 homework', 'Week 15 homework']\n",
    "\n",
    "columns_name_student = \"Student\"\n",
    "columns_name_score = \"Score\"\n",
    "columns_name_grade = \"Grade\"\n",
    "\n",
    "columns_name_activity = \"Activity\"\n",
    "columns_name_day = \"Day\"\n",
    "columns_name_time = \"Time\"\n",
    "add_column_activity_weeknumber = \"Activity_Num\"\n",
    "add_column_activity_score = \"Activity Score\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# in data_df, subsititute - with NaN in column score\n",
    "data_df[columns_name_score] = data_df[columns_name_score].replace('-', np.nan)\n",
    "data_df[columns_name_score] = pd.to_numeric(data_df[columns_name_score], errors='coerce')\n",
    "\n",
    "# in data_df, delete the data that does not participate in any activity or got F due to other activtiy\n",
    "# in data_df, delete the data that got A due to pre-exam\n",
    "data_df = data_df.dropna(subset=[columns_name_score])\n",
    "data_df = data_df.drop(data_df[(data_df[columns_name_score] > 50) & (data_df[columns_name_grade] == 'F')].index)\n",
    "\n",
    "conditions = (data_df[columns_with_missing_values].eq(0).all(axis=1)) & (data_df[columns_name_grade] == 'A')\n",
    "data_df = data_df[~conditions]\n",
    "\n",
    "# in log_df, delete the data after ddl\n",
    "log_df[columns_name_time] = pd.to_datetime(log_df[columns_name_time])\n",
    "log_df[add_column_activity_weeknumber] = log_df[columns_name_activity].str.extract(r\"(\\d+)\")\n",
    "log_df[add_column_activity_weeknumber] = log_df[add_column_activity_weeknumber].fillna(0)\n",
    "log_df[add_column_activity_weeknumber] = log_df[add_column_activity_weeknumber].astype(int)\n",
    "\n",
    "week1 = (log_df[add_column_activity_weeknumber] == 1)\n",
    "week2_16 = (log_df[add_column_activity_weeknumber] > 1) & (log_df[add_column_activity_weeknumber] <= 16)\n",
    "\n",
    "time_start_for_week2_16 = (log_df[columns_name_time].dt.time < pd.to_datetime('18:00:00').time())\n",
    "time_end_for_week2_16 = (log_df[columns_name_time].dt.time > pd.to_datetime('8:00:00').time())\n",
    "\n",
    "invalid_for_week1 = (log_df[columns_name_day] < 1) | (log_df[columns_name_day] > 7)\n",
    "\n",
    "invalid_start_for_week2_16 = (log_df[columns_name_day] < (7 * (log_df[add_column_activity_weeknumber] -1) - 3)) | ((log_df[columns_name_day] == (7 * (log_df[add_column_activity_weeknumber] -1) - 3)) & time_start_for_week2_16)\n",
    "invalid_end_for_week2_16 = (log_df[columns_name_day] > (7 * (log_df[add_column_activity_weeknumber] -1) + 4)) | ((log_df[columns_name_day] == (7 * (log_df[add_column_activity_weeknumber] -1) + 4)) & time_end_for_week2_16)\n",
    "\n",
    "condition1 = week1 & invalid_for_week1\n",
    "condition2 = week2_16 & (invalid_start_for_week2_16 | invalid_end_for_week2_16)\n",
    "\n",
    "log_df = log_df.drop(log_df[condition1].index)\n",
    "if not log_df.empty:\n",
    "    log_df = log_df.drop(log_df[condition2].index)\n",
    "log_df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "log_df = log_df.drop(add_column_activity_weeknumber, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# in log_df, build new column Score_First and Score_Last for first score and last score respectively\n",
    "suffixes_first = '_First'\n",
    "suffixes_last = '_Last'\n",
    "columns_name_first_score = columns_name_score + suffixes_first\n",
    "columns_name_last_score = columns_name_score + suffixes_last\n",
    "columns_name_first_day = columns_name_day + suffixes_first\n",
    "columns_name_last_day = columns_name_day + suffixes_last\n",
    "columns_name_first_time = columns_name_time + suffixes_first\n",
    "columns_name_last_time = columns_name_time + suffixes_last\n",
    "\n",
    "log_df[columns_name_day] = log_df[columns_name_day].astype(int)\n",
    "log_df_sorted = log_df.sort_values([columns_name_student, columns_name_activity, columns_name_day, columns_name_time])\n",
    "\n",
    "first_scores = log_df_sorted.groupby([columns_name_student, columns_name_activity]).first()\n",
    "last_scores = log_df_sorted.groupby([columns_name_student, columns_name_activity]).last()\n",
    "\n",
    "log_df = pd.merge(log_df, first_scores, on=[columns_name_student, columns_name_activity], suffixes=('', suffixes_first))\n",
    "log_df = pd.merge(log_df, last_scores, on=[columns_name_student, columns_name_activity], suffixes=('', suffixes_last))\n",
    "\n",
    "log_df = log_df.drop(columns_name_day, axis=1)\n",
    "log_df = log_df.drop(columns_name_time, axis=1)\n",
    "log_df = log_df.drop(columns_name_first_day, axis=1)\n",
    "log_df = log_df.drop(columns_name_last_day, axis=1)\n",
    "log_df = log_df.drop(columns_name_first_time, axis=1)\n",
    "log_df = log_df.drop(columns_name_last_time, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# in log_df, detele all the data without maximum score\n",
    "max_score_df = log_df.groupby([columns_name_student, columns_name_activity, columns_name_first_score, columns_name_last_score])[columns_name_score].max().reset_index()\n",
    "log_df = log_df.merge(max_score_df, on=[columns_name_student, columns_name_activity, columns_name_first_score, columns_name_last_score, columns_name_score], how='inner')\n",
    "log_df.drop_duplicates(subset=[columns_name_student, columns_name_activity, columns_name_first_score, columns_name_last_score], keep='first', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# merge data and log based on student and activity\n",
    "# Pivot log_df to get the columns for max score, first score, and last score\n",
    "max_score_df = log_df.pivot(index='Student', columns='Activity', values='Score').add_suffix('_maxscore')\n",
    "first_score_df = log_df.pivot(index='Student', columns='Activity', values='Score_First').add_suffix('_firstscore')\n",
    "last_score_df = log_df.pivot(index='Student', columns='Activity', values='Score_Last').add_suffix('_lastscore')\n",
    "\n",
    "# Merge the pivoted DataFrames with data_df\n",
    "merged_df = data_df.merge(max_score_df, left_on='Student', right_index=True, how='left')\n",
    "merged_df = merged_df.merge(first_score_df, left_on='Student', right_index=True, how='left')\n",
    "merged_df = merged_df.merge(last_score_df, left_on='Student', right_index=True, how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_df.to_csv('/content/data_df.csv', index=False)\n",
    "files.download('/content/data_df.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_df.to_csv('/content/log_df.csv', index=False)\n",
    "files.download('/content/log_df.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df.to_csv('/content/merged_df.csv', index=False)\n",
    "files.download('/content/merged_df.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# define three sets of features\n",
    "# first set: the number of attempts and maximum score of each exercise\n",
    "features = ['Week 1 quiz', 'Week 2 quiz', 'Week 3 quiz', \n",
    "            'Week 4 quiz', 'Week 5 quiz', 'Week 6 quiz', 'Week 7 quiz', 'Week 8 quiz', 'Week 9 quiz', \n",
    "            'Week 10 quiz', 'Week 11 quiz', 'Week 12 quiz', 'Week 13 quiz', 'Week 14 quiz', \n",
    "            'Week 15 quiz', 'Week 16 quiz', 'Week 1 homework', 'Week 2 homework', \n",
    "            'Week 3 homework', 'Week 4 homework', 'Week 5 homework', \n",
    "            'Week 7 homework', 'Week 8 homework', 'Week 9 homework', 'Week 10 homework', 'Week 11 homework', \n",
    "            'Week 13 homework', 'Week 14 homework', 'Week 15 homework', 'Week 1 homework_maxscore', \n",
    "            'Week 1 quiz_maxscore', 'Week 10 homework_maxscore', 'Week 10 quiz_maxscore', 'Week 11 homework_maxscore', \n",
    "            'Week 11 quiz_maxscore', 'Week 12 quiz_maxscore', 'Week 13 homework_maxscore', 'Week 13 quiz_maxscore', \n",
    "            'Week 14 homework_maxscore', 'Week 14 quiz_maxscore', 'Week 15 homework_maxscore', 'Week 15 quiz_maxscore', \n",
    "            'Week 16 quiz_maxscore', 'Week 2 homework_maxscore', 'Week 2 quiz_maxscore', 'Week 3 homework_maxscore', \n",
    "            'Week 3 quiz_maxscore', 'Week 4 homework_maxscore', 'Week 4 quiz_maxscore', 'Week 5 homework_maxscore', \n",
    "            'Week 5 quiz_maxscore', 'Week 6 quiz_maxscore', 'Week 7 homework_maxscore', 'Week 7 quiz_maxscore', \n",
    "            'Week 8 homework_maxscore', 'Week 8 quiz_maxscore', 'Week 9 homework_maxscore', 'Week 9 quiz_maxscore']\n",
    "\n",
    "# second set: maximum score of each exercise\n",
    "features_new = ['Week 1 homework_maxscore', \n",
    "            'Week 1 quiz_maxscore', 'Week 10 homework_maxscore', 'Week 10 quiz_maxscore', 'Week 11 homework_maxscore', \n",
    "            'Week 11 quiz_maxscore', 'Week 12 quiz_maxscore', 'Week 13 homework_maxscore', 'Week 13 quiz_maxscore', \n",
    "            'Week 14 homework_maxscore', 'Week 14 quiz_maxscore', 'Week 15 homework_maxscore', 'Week 15 quiz_maxscore', \n",
    "            'Week 16 quiz_maxscore', 'Week 2 homework_maxscore', 'Week 2 quiz_maxscore', 'Week 3 homework_maxscore', \n",
    "            'Week 3 quiz_maxscore', 'Week 4 homework_maxscore', 'Week 4 quiz_maxscore', 'Week 5 homework_maxscore', \n",
    "            'Week 5 quiz_maxscore', 'Week 6 quiz_maxscore', 'Week 7 homework_maxscore', 'Week 7 quiz_maxscore', \n",
    "            'Week 8 homework_maxscore', 'Week 8 quiz_maxscore', 'Week 9 homework_maxscore', 'Week 9 quiz_maxscore']\n",
    "\n",
    "# third set: the number of attempts, first and last score of each exercise\n",
    "features_third = ['Week 1 quiz', 'Week 2 quiz', 'Week 3 quiz', \n",
    "            'Week 4 quiz', 'Week 5 quiz', 'Week 6 quiz', 'Week 7 quiz', 'Week 8 quiz', 'Week 9 quiz', \n",
    "            'Week 10 quiz', 'Week 11 quiz', 'Week 12 quiz', 'Week 13 quiz', 'Week 14 quiz', \n",
    "            'Week 15 quiz', 'Week 16 quiz', 'Week 1 homework', 'Week 2 homework', \n",
    "            'Week 3 homework', 'Week 4 homework', 'Week 5 homework', \n",
    "            'Week 7 homework', 'Week 8 homework', 'Week 9 homework', 'Week 10 homework', 'Week 11 homework', \n",
    "            'Week 13 homework', 'Week 14 homework', 'Week 15 homework', 'Week 1 homework_firstscore', \n",
    "            'Week 1 quiz_firstscore', 'Week 10 homework_firstscore', \n",
    "            'Week 10 quiz_firstscore', 'Week 11 homework_firstscore', 'Week 11 quiz_firstscore', \n",
    "            'Week 12 quiz_firstscore', 'Week 13 homework_firstscore', 'Week 13 quiz_firstscore', \n",
    "            'Week 14 homework_firstscore', 'Week 14 quiz_firstscore', 'Week 15 homework_firstscore', \n",
    "            'Week 15 quiz_firstscore', 'Week 16 quiz_firstscore', 'Week 2 homework_firstscore', \n",
    "            'Week 2 quiz_firstscore', 'Week 3 homework_firstscore', 'Week 3 quiz_firstscore', \n",
    "            'Week 4 homework_firstscore', 'Week 4 quiz_firstscore', 'Week 5 homework_firstscore', \n",
    "            'Week 5 quiz_firstscore', 'Week 6 quiz_firstscore', 'Week 7 homework_firstscore', \n",
    "            'Week 7 quiz_firstscore', 'Week 8 homework_firstscore', 'Week 8 quiz_firstscore', \n",
    "            'Week 9 homework_firstscore', 'Week 9 quiz_firstscore', 'Week 1 homework_lastscore', \n",
    "            'Week 1 quiz_lastscore', 'Week 10 homework_lastscore', 'Week 10 quiz_lastscore', \n",
    "            'Week 11 homework_lastscore', 'Week 11 quiz_lastscore', 'Week 12 quiz_lastscore', \n",
    "            'Week 13 homework_lastscore', 'Week 13 quiz_lastscore', 'Week 14 homework_lastscore', \n",
    "            'Week 14 quiz_lastscore', 'Week 15 homework_lastscore', 'Week 15 quiz_lastscore', \n",
    "            'Week 16 quiz_lastscore', 'Week 2 homework_lastscore', 'Week 2 quiz_lastscore', \n",
    "            'Week 3 homework_lastscore', 'Week 3 quiz_lastscore', 'Week 4 homework_lastscore', \n",
    "            'Week 4 quiz_lastscore', 'Week 5 homework_lastscore', 'Week 5 quiz_lastscore', \n",
    "            'Week 6 quiz_lastscore', 'Week 7 homework_lastscore', 'Week 7 quiz_lastscore', \n",
    "            'Week 8 homework_lastscore', 'Week 8 quiz_lastscore', 'Week 9 homework_lastscore', 'Week 9 quiz_lastscore']\n",
    "\n",
    "# define target：to predict the final course score\n",
    "target = 'Score'\n",
    "\n",
    "# dataset regularization\n",
    "# split the data： 80%/20%\n",
    "merged_df[features] = merged_df[features].fillna(value=0).replace('-', 0).astype(float)\n",
    "merged_df[features_third] = merged_df[features_third].fillna(value=0).replace('-', 0).astype(float)\n",
    "merged_df[columns_name_score] = merged_df[columns_name_score].fillna(value=0).replace('-', 0).astype(float)\n",
    "\n",
    "train_df, test_df, train_labels, test_labels = train_test_split(merged_df[features],\n",
    "                                                                merged_df[target],\n",
    "                                                                test_size=0.2,\n",
    "                                                                random_state=42)\n",
    "\n",
    "train_df_new, test_df_new, train_labels_new, test_labels_new = train_test_split(merged_df[features_new],\n",
    "                                                                merged_df[target],\n",
    "                                                                test_size=0.2,\n",
    "                                                                random_state=42)\n",
    "\n",
    "train_df_third, test_df_third, train_labels_third, test_labels_third = train_test_split(merged_df[features_third],\n",
    "                                                                merged_df[target],\n",
    "                                                                test_size=0.2,\n",
    "                                                                random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "# initical the models\n",
    "linear_model = LinearRegression()\n",
    "tree_model = DecisionTreeRegressor()\n",
    "forest_model = RandomForestRegressor()\n",
    "\n",
    "# perform model training\n",
    "linear_model.fit(train_df, train_labels)\n",
    "tree_model.fit(train_df, train_labels)\n",
    "forest_model.fit(train_df, train_labels)\n",
    "\n",
    "# predictions on the test set\n",
    "linear_predictions = linear_model.predict(test_df)\n",
    "tree_predictions = tree_model.predict(test_df)\n",
    "forest_predictions = forest_model.predict(test_df)\n",
    "\n",
    "# second set of models\n",
    "linear_model_new = LinearRegression()\n",
    "tree_model_new = DecisionTreeRegressor()\n",
    "forest_model_new = RandomForestRegressor()\n",
    "\n",
    "linear_model_new.fit(train_df_new, train_labels_new)\n",
    "tree_model_new.fit(train_df_new, train_labels_new)\n",
    "forest_model_new.fit(train_df_new, train_labels_new)\n",
    "\n",
    "linear_predictions_new = linear_model_new.predict(test_df_new)\n",
    "tree_predictions_new = tree_model_new.predict(test_df_new)\n",
    "forest_predictions_new = forest_model_new.predict(test_df_new)\n",
    "\n",
    "# third set of models\n",
    "linear_model_third = LinearRegression()\n",
    "tree_model_third = DecisionTreeRegressor()\n",
    "forest_model_third = RandomForestRegressor()\n",
    "\n",
    "linear_model_third.fit(train_df_third, train_labels_third)\n",
    "tree_model_third.fit(train_df_third, train_labels_third)\n",
    "forest_model_third.fit(train_df_third, train_labels_third)\n",
    "\n",
    "linear_predictions_third = linear_model_third.predict(test_df_third)\n",
    "tree_predictions_third = tree_model_third.predict(test_df_third)\n",
    "forest_predictions_third = forest_model_third.predict(test_df_third)\n",
    "\n",
    "# Compare the effect of each model\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "\n",
    "# Mean Squared Error\n",
    "linear_mse = mean_squared_error(test_labels, linear_predictions)\n",
    "tree_mse = mean_squared_error(test_labels, tree_predictions)\n",
    "forest_mse = mean_squared_error(test_labels, forest_predictions)\n",
    "\n",
    "# Mean Absolute Error\n",
    "linear_mae = mean_absolute_error(test_labels, linear_predictions)\n",
    "tree_mae = mean_absolute_error(test_labels, tree_predictions)\n",
    "forest_mae = mean_absolute_error(test_labels, forest_predictions)\n",
    "\n",
    "# Print the compare results\n",
    "print(\"Mean Squared Error for linear_model: \", linear_mse, \"Mean Squared Error for tree_model: \", tree_mse, \"Mean Squared Error for forest_model: \", forest_mse)\n",
    "print(\"Mean Absolute Error for linear_model: \", linear_mae, \"Mean Absolute Error for tree_model: \", tree_mae, \"Mean Absolute Error for forest_model: \", forest_mae)\n",
    "\n",
    "# new Mean Squared Error\n",
    "linear_mse_new = mean_squared_error(test_labels_new, linear_predictions_new)\n",
    "tree_mse_new = mean_squared_error(test_labels_new, tree_predictions_new)\n",
    "forest_mse_new = mean_squared_error(test_labels_new, forest_predictions_new)\n",
    "\n",
    "# new Mean Absolute Error\n",
    "linear_mae_new = mean_absolute_error(test_labels_new, linear_predictions_new)\n",
    "tree_mae_new = mean_absolute_error(test_labels_new, tree_predictions_new)\n",
    "forest_mae_new = mean_absolute_error(test_labels_new, forest_predictions_new)\n",
    "\n",
    "# Print the new compare results\n",
    "print(\"Mean Squared Error for new linear_model: \", linear_mse_new, \"Mean Squared Error for new tree_model: \", tree_mse_new, \"Mean Squared Error for new forest_model: \", forest_mse_new)\n",
    "print(\"Mean Absolute Error for new linear_model: \", linear_mae_new, \"Mean Absolute Error for new tree_model: \", tree_mae_new, \"Mean Absolute Error for new forest_model: \", forest_mae_new)\n",
    "\n",
    "# third Mean Squared Error\n",
    "linear_mse_third = mean_squared_error(test_labels_third, linear_predictions_third)\n",
    "tree_mse_third = mean_squared_error(test_labels_third, tree_predictions_third)\n",
    "forest_mse_third = mean_squared_error(test_labels_third, forest_predictions_third)\n",
    "\n",
    "# third Mean Absolute Error\n",
    "linear_mae_third = mean_absolute_error(test_labels_third, linear_predictions_third)\n",
    "tree_mae_third = mean_absolute_error(test_labels_third, tree_predictions_third)\n",
    "forest_mae_third = mean_absolute_error(test_labels_third, forest_predictions_third)\n",
    "\n",
    "# Print the third compare results\n",
    "print(\"Mean Squared Error for third linear_model: \", linear_mse_third, \"Mean Squared Error for third tree_model: \", tree_mse_third, \"Mean Squared Error for third forest_model: \", forest_mse_third)\n",
    "print(\"Mean Absolute Error for third linear_model: \", linear_mae_third, \"Mean Absolute Error for third tree_model: \", tree_mae_third, \"Mean Absolute Error for forest_model: \", forest_mae_third)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVR\n",
    "\n",
    "svm_model = SVR()\n",
    "svm_model.fit(train_df, train_labels)\n",
    "svm_predictions = svm_model.predict(test_df)\n",
    "\n",
    "svm_mse = mean_squared_error(test_labels, svm_predictions)\n",
    "svm_mae = mean_absolute_error(test_labels, svm_predictions)\n",
    "\n",
    "svm_model_2 = SVR()\n",
    "svm_model_2.fit(train_df_new, train_labels_new)\n",
    "svm_predictions_2 = svm_model_2.predict(test_df_new)\n",
    "\n",
    "svm_mse_2 = mean_squared_error(test_labels, svm_predictions_2)\n",
    "svm_mae_2 = mean_absolute_error(test_labels, svm_predictions_2)\n",
    "\n",
    "svm_model_3 = SVR()\n",
    "svm_model_3.fit(train_df_third, train_labels_third)\n",
    "svm_predictions_third = svm_model_3.predict(test_df_third)\n",
    "\n",
    "svm_mse_3 = mean_squared_error(test_labels_third, svm_predictions_third)\n",
    "svm_mae_3 = mean_absolute_error(test_labels_third, svm_predictions_third)\n",
    "\n",
    "print(\"Mean Squared Error for svm_model 1/2/3: \", svm_mse, svm_mse_2, svm_mse_3)\n",
    "print(\"Mean Absolute Error for svm_model 1/2/3: \", svm_mae, svm_mae_2, svm_mae_3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neural_network import MLPRegressor\n",
    "\n",
    "mlp_model = MLPRegressor()\n",
    "mlp_model.fit(train_df, train_labels)\n",
    "mlp_predictions = mlp_model.predict(test_df)\n",
    "\n",
    "mlp_mse = mean_squared_error(test_labels, mlp_predictions)\n",
    "mlp_mae = mean_absolute_error(test_labels, mlp_predictions)\n",
    "\n",
    "mlp_model_2 = MLPRegressor()\n",
    "mlp_model_2.fit(train_df_new, train_labels_new)\n",
    "mlp_predictions_2 = mlp_model_2.predict(test_df_new)\n",
    "\n",
    "mlp_mse_2 = mean_squared_error(test_labels_new, mlp_predictions_2)\n",
    "mlp_mae_2 = mean_absolute_error(test_labels_new, mlp_predictions_2)\n",
    "\n",
    "mlp_model_3 = MLPRegressor()\n",
    "mlp_model_3.fit(train_df_third, train_labels_third)\n",
    "mlp_predictions_3 = mlp_model_3.predict(test_df_third)\n",
    "\n",
    "mlp_mse_3 = mean_squared_error(test_labels_third, mlp_predictions_3)\n",
    "mlp_mae_3 = mean_absolute_error(test_labels_third, mlp_predictions_3)\n",
    "\n",
    "print(\"Mean Squared Error for mlp_model 1/2/3: \", mlp_mse, mlp_mse_2, mlp_mse_3)\n",
    "print(\"Mean Absolute Error for mlp_model 1/2/3: \", mlp_mae, mlp_mae_2, mlp_mae_3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from xgboost import XGBRegressor\n",
    "\n",
    "xgb_model = XGBRegressor()\n",
    "xgb_model.fit(train_df, train_labels)\n",
    "\n",
    "xgb_predictions = xgb_model.predict(test_df)\n",
    "xgb_mse = mean_squared_error(test_labels, xgb_predictions)\n",
    "xgb_mae = mean_absolute_error(test_labels, xgb_predictions)\n",
    "\n",
    "xgb_model_2 = XGBRegressor()\n",
    "xgb_model_2.fit(train_df_new, train_labels_new)\n",
    "\n",
    "xgb_predictions_new = xgb_model_2.predict(test_df_new)\n",
    "xgb_mse_2 = mean_squared_error(test_labels_new, xgb_predictions_new)\n",
    "xgb_mae_2 = mean_absolute_error(test_labels_new, xgb_predictions_new)\n",
    "\n",
    "xgb_model_3 = XGBRegressor()\n",
    "xgb_model_3.fit(train_df_third, train_labels_third)\n",
    "\n",
    "xgb_predictions_3 = xgb_model_3.predict(test_df_third)\n",
    "xgb_mse_3 = mean_squared_error(test_labels_third, xgb_predictions_3)\n",
    "xgb_mae_3 = mean_absolute_error(test_labels_third, xgb_predictions_3)\n",
    "\n",
    "# Print the MSE and MAE\n",
    "print(\"Mean Squared Error for XGBoost model:\", xgb_mse, xgb_mse_2, xgb_mse_3)\n",
    "print(\"Mean Absolute Error for XGBoost model:\", xgb_mae, xgb_mae_2, xgb_mae_3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for second subtask\n",
    "# difine sum of attempts for every activty, sum of max score for every activty\n",
    "add_column_attempts_sum = \"attempts_sum\"\n",
    "add_column_maxscore_sum = \"maxscore_sum\"\n",
    "add_column_cluster_label = \"cluster_label\"\n",
    "\n",
    "merged_df[add_column_attempts_sum] = merged_df[['Week 1 quiz', 'Week 2 quiz', 'Week 3 quiz',\n",
    "            'Week 4 quiz', 'Week 5 quiz', 'Week 6 quiz', 'Week 7 quiz', 'Week 8 quiz', 'Week 9 quiz',\n",
    "            'Week 10 quiz', 'Week 11 quiz', 'Week 12 quiz', 'Week 13 quiz', 'Week 14 quiz',\n",
    "            'Week 15 quiz', 'Week 16 quiz', 'Week 1 homework', 'Week 2 homework',\n",
    "            'Week 3 homework', 'Week 4 homework', 'Week 5 homework',\n",
    "            'Week 7 homework', 'Week 8 homework', 'Week 9 homework', 'Week 10 homework', 'Week 11 homework',\n",
    "            'Week 13 homework', 'Week 14 homework', 'Week 15 homework']].sum(axis=1)\n",
    "\n",
    "merged_df[add_column_maxscore_sum] = merged_df[['Week 1 homework_maxscore',\n",
    "            'Week 1 quiz_maxscore', 'Week 10 homework_maxscore', 'Week 10 quiz_maxscore', 'Week 11 homework_maxscore',\n",
    "            'Week 11 quiz_maxscore', 'Week 12 quiz_maxscore', 'Week 13 homework_maxscore', 'Week 13 quiz_maxscore',\n",
    "            'Week 14 homework_maxscore', 'Week 14 quiz_maxscore', 'Week 15 homework_maxscore', 'Week 15 quiz_maxscore',\n",
    "            'Week 16 quiz_maxscore', 'Week 2 homework_maxscore', 'Week 2 quiz_maxscore', 'Week 3 homework_maxscore',\n",
    "            'Week 3 quiz_maxscore', 'Week 4 homework_maxscore', 'Week 4 quiz_maxscore', 'Week 5 homework_maxscore',\n",
    "            'Week 5 quiz_maxscore', 'Week 6 quiz_maxscore', 'Week 7 homework_maxscore', 'Week 7 quiz_maxscore',\n",
    "            'Week 8 homework_maxscore', 'Week 8 quiz_maxscore', 'Week 9 homework_maxscore', 'Week 9 quiz_maxscore']].sum(axis=1)\n",
    "\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import silhouette_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# choose features: sum of attempts, sum of max score for every activity, course final score\n",
    "X = merged_df[[add_column_attempts_sum, add_column_maxscore_sum, columns_name_score]].values\n",
    "y = merged_df[columns_name_student].values\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# unsupervised cluster analysis\n",
    "best_k = -1\n",
    "best_silhouette_score = -1\n",
    "\n",
    "k_values = range(2, 11)\n",
    "silhouette_scores = []\n",
    "\n",
    "for k in k_values:\n",
    "    kmeans = KMeans(n_clusters=k, random_state=42)\n",
    "    labels = kmeans.fit_predict(X_scaled)\n",
    "    silhouette_avg = silhouette_score(X_scaled, labels)\n",
    "    silhouette_scores.append(silhouette_avg)\n",
    "    if silhouette_avg > best_silhouette_score:\n",
    "        best_silhouette_score = silhouette_avg\n",
    "        best_k = k\n",
    "\n",
    "print(f\"best_k: {best_k}\")\n",
    "# Plotting contour coefficients versus K-values\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(k_values, silhouette_scores, marker='o')\n",
    "plt.xlabel('K')\n",
    "plt.ylabel('Silhouette Score')\n",
    "plt.title('Silhouette Score for each K')\n",
    "plt.xticks(k_values)\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "# Clustering using optimal K-values and save cluster labels\n",
    "kmeans = KMeans(n_clusters=best_k, random_state=42)\n",
    "labels = kmeans.fit_predict(X_scaled)\n",
    "\n",
    "merged_df[add_column_cluster_label] = labels\n",
    "\n",
    "# Supervised learning for classification\n",
    "models = {\n",
    "    'Logistic Regression': LogisticRegression(),\n",
    "    'K-Nearest Neighbors': KNeighborsClassifier(),\n",
    "    'Support Vector Machine': SVC(),\n",
    "    'Decision Tree': DecisionTreeClassifier()\n",
    "}\n",
    "\n",
    "best_model = None\n",
    "best_accuracy = -1\n",
    "\n",
    "accuracies = []\n",
    "model_names = []\n",
    "\n",
    "for model_name, model in models.items():\n",
    "    model.fit(X, y)\n",
    "    predicted_labels = model.predict(X)\n",
    "    accuracy = accuracy_score(y, predicted_labels)\n",
    "    model_names.append(model_name)\n",
    "    accuracies.append(accuracy)\n",
    "    if accuracy > best_accuracy:\n",
    "        best_accuracy = accuracy\n",
    "        best_model = model_name\n",
    "\n",
    "    # Adding classification results to raw data\n",
    "    merged_df[model_name] = predicted_labels\n",
    "\n",
    "    # Calculation of contour coefficients\n",
    "    silhouette_avg = silhouette_score(X_scaled, predicted_labels)\n",
    "\n",
    "    print(f\"{model_name}'s accuracy: {accuracy}\")\n",
    "    print(f\"{model_name}'s Calculation of contour coefficients: {silhouette_avg}\")\n",
    "\n",
    "print(f\"best_model: {best_model}\")\n",
    "# Charting model accuracy\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.bar(model_names, accuracies)\n",
    "plt.xlabel('Model')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.title('Accuracy for each Model')\n",
    "# plt.ylim([0, 1])\n",
    "plt.show()\n",
    "\n",
    "0# download the final \n",
    "merged_df.to_csv('merged_df_classified_data.csv', index=False)\n",
    "files.download('/content/merged_df_classified_data.csv')"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
